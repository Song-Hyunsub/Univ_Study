(오차 제곱합 : 배치 내의 네트워크 출력과 정답 간의 오차를 제곱해서 모두 더하는 방식의 손실 함수)  
  
## 1. 실습을 위한 회귀문제 조건 설정  
  
[1] 문제  
 - sin 함수  
  + x값을 입력, y값을 출력, sin(x)를 정답으로 함  
  + 연속된 값을 구하는 문제이므로 회귀(Regression)에 해당  
  
[2] 조건 설정  
 - 입력층 뉴런 1개, 은닉층 뉴런 3개, 출력층 뉴런 1개  
 - 은닉층 활성화 함수 : 시그모이드  
 - 출력층 활성화 함수 : 항등 함수  
 - 손실 함수 : 오차 제곱합  
 - 최적화 알고리즘 : SGD  
 - 배치 사이즈 : 1  
  
- 일반적으로 초기 은닉층은 입력의 2배 정도의 뉴런으로 출발을 하면서 점차 줄여 나가는 방법을 구사한다.  
  
## 2. 파이썬을 이용한 회귀문제 딥러닝 구현  
  
 [1] 구현  
  - 모든 데이터를 훈련 데이터로 사용!!  
  - 구현을 위해 은닉층과 출력층을 클래스로 구현하고, 이전에 준비한 행력식으로 로직을 구현한다.  
  
 [2] 코드   
import numpy as np  
import matplotlib.pyplot as plt  
  
input_data = np.arange(0, np.pi*2, 0.1)  # 0부터 2pi 전까지 0.1 간격으로 값 생성  
correct_data = np.sin(input_data)  # sin 값을 실제 정답으로 활용  
input_data = (input_data - np.pi)/np.pi # 전체 입력값의 범위가 -1~1로 조정됨!   
n_data = len(correct_data)  # 전체 데이터(샘플)의 개수  
  
n_in = 1  
n_mid = 3  
n_out = 1  
  
wb_width = 0.01  # 가중치와 편향을 초기화할 때 사용할 표준편차(정규분포) 규모  
eta = 0.1  # 학습률  
epoch = 2001  # 총 학습 반복 횟수  
interval = 200  # 학습 진행 상황을 출력하고 그래프를 그릴 주기  
  
class MiddleLayer:  
  def __init__(self, n_upper, n):  # n_upper : 바로 이전 층의 뉴런 개수, n : 현재 층(은닉층)의 뉴런 개수  
    # 은닉층 가중치와 편향을 무작위로 초기화합니다.
    self.w = wb_width * np.random.randn(n_upper, n)  # 평균 0, 표준편차 wb_width인 정규분포로 초기화  
    self.b = wb_width * np.random.randn(n)  
  def forward(self, x):  # 입력 x(배치 형태)를 받아 u = xW + b 계산 후, 시그모이드 함수를 적용하여 은닉층 출력을 구함  
    self.x = x  
    u = np.dot(x, self.w) + self.b  
    self.y = 1 / (1+np.exp(-u))  # 시그모이드 활성화 함수  
  def backward(self, grad_y):  # 출력층(또는 다음 층)에서 전해진 오차 기울기 grad_y를 사용하여 은닉층의 가중치/편향에 대한 기울기를 계산한다  
    # 미분 결과 δ를 구한 뒤, 이를 이용해 가중치 기울기 self.grad_w, 편향 기울기 self.grad_b, 입력방향 기울기 self.grad_x(이전 층으로 전파할 오차 기울기)를 구한다  
    # grad_y: 다음 층(출력층)에서 전해진 오차 기울기  
    delta = grad_y * (1 - self.y) * self.y  # 시그모이드 미분  
    self.grad_w = np.dot(self.x.T, delta)  
    self.grad_b = np.sum(delta, axis=0)  
    self.grad_x = np.dot(delta, self.w.T)  
  def update(self, eta):  # 경사하강법으로 가중치와 편향을 수정한다  
    self.w -= eta * self.grad_w  
    self.b -= eta * self.grad_b  
  
class OutputLayer:  # 은닉층과 달리 추가적인 활성화 함수(시그모이드, ReLU 등)를 쓰지 않고, y = u 그대로 내보낸다.(선형 출력)  
  def __init__(self, n_upper, n):  
    self.w = wb_width * np.random.randn(n_upper, n)  
    self.b = wb_width * np.random.randn(n)  
  def forward(self, x):  
    self.x = x  
    u = np.dot(x, self.w) + self.b  
    self.y = u  
  def backward(self, t):  # 예측치 self.y와 실제값 t의 오차 delta = y - t를 구해, 가중치와 편향의 기울기 계산  
    delta = self.y - t  
    self.grad_w = np.dot(self.x.T, delta)  
    self.grad_b = np.sum(delta, axis=0)  
    self.grad_x = np.dot(delta, self.w.T)  # 이전 층(은닉층)으로 전달할 오차 기울기
  def update(self, eta):  
    self.w -= eta * self.grad_w  
    self.b -= eta * self.grad_b  

\# 앞서 정의한 클래스 은닉층, 출력층을 생성  
\# 입력층->은닉층(3뉴런)->출력층(1뉴런) 구조 완성  
middle_layer = MiddleLayer(n_in, n_mid) 
output_layer = OutputLayer(n_mid, n_out)  
  
for i in range(epoch):  
  index_random = np.arange(n_data)  
  np.random.shuffle(index_random)  
  
  total_error = 0  
  plot_x = []  
  plot_y = []  
  
  for idx in index_random:  
    x = input_data[idx]  
    t = correct_data[idx]  
  
    middle_layer.forward(x.reshape(1, 1))  
    output_layer.forward(middle_layer.y)  
  
    output_layer.backward(t.reshape(1, 1))  
    middle_layer.backward(output_layer.grad_x)  
  
    middle_layer.update(eta)  
    output_layer.update(eta)  
  
    if i % interval == 0:  
      y = output_layer.y.reshape(-1)  
      total_error = 1.0/2.0 * np.sum(np.square(y - t))  
  
      plot_x.append(x)  
      plot_y.append(y)  
      
  if i % interval == 0:  
    plt.plot(input_data, correct_data, linestyle='dashed')  
    plt.scatter(plot_x, plot_y, marker='+')  
    plt.show()  
  
    print('Epoch: ' + str(i) + '/' + str(epoch), 'Error: ' + str(total_error / n_data))  
