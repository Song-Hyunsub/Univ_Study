1. 오차에 대한 기울기를 구하는 순서  
  
[1] 오차에 대한 기울기  
 - 순전파를 통해 각 층의 출력을 구함  
 - 최종 출력과 정답간의 오차를 구함  
 - 최종 오차에 대한 각 층의 가중치 및 편향들의 기울기 계산  
  + [기울기를 구하는 신경망] 사진 참고!  
  (오차에 대한 기울기는 마지막 층부터 계산하면서 거슬러 올라가야 함!!)
입력층 : 첨자 - i, 뉴런 수 - l  
은닉층 : 첨자 - j, 뉴런 수 - m  
출력층 : 첨자 - k, 뉴런 수 - n  
  
2. 출력층 파라미터의 기울기 계산  
  
[1] 출력층 뉴런  
 - y_j : 은닉층의 j번째 출력  
 - w_jk : 가중치 (j, k)  
 - b_k : k번째 편향  
 - u_k : k번째 가중치곱과 편향의 합  
 - y_k : k번째 출력  
입력층과 출력층 사이에 관여하는 가중치 파라미터 W의 shape은? : m x n
 Y = [y_1, y_2, ..., y_j, ..., y_m]  
 W =  
w_11 w_12 ... w_1n  
w_21 w_22 ... w_2n  
...  
w_m1 w_m2 ... w_mn  
 
입력 m개, 출력 n개  
  
 [2] 가중치 기울기  
  - 오차 E를 가중치 w_jk로 편미분한 d E / d w_jk를 구함  
  - 이렇게 구한 가중치 기울기를 d w_jk라 하면 다음과 같이 표기  
   d w_jk = d E / d w_jk  
  - 위 식을 합성함수 미분법칙을 사용하여 다음과 같이 전개  
   d w_jk = d E / d w_jk = (d E / d u_k) * (d u_k / d w_jk)  
  - 우변 d u_k / d w_jk의 분자 u_k는 은닉층에 있는 여러 뉴런의 출력값과 가중치 곱의 합에 편향을 더한 것  
   d u_k / d w_jk = d (sum(from q=1 to m)(y_q * w_qk + b_k)) / d w_jk  
   = d / d w_jk * (y_1*w_1k + y_2*w_2k + ... + y_j*w_jk + y_m*w_mk + b_k)  
   = y_j  --- (2)
  - d w_jk = d E / d w_jk = (d E / d u_k) * (d u_k / d w_jk) ----(1)
    여기서  d E / d u_k = (d E / d y_k) * (d y_k / d u_k) 가 됨  
  - 즉, 오차를 출력층 뉴런의 출력으로 편미분한 것과 그 출력값을 u_k로 편미분한 것의 곱으로 표현  
  - 이때, d E / d y_k : 손실 함수를 편미분해서 구함  
  - d y_k / d u_k : 활성화 함수를 편미분해서 구함   
  - 위 결과를 delta_k라 하면 다음과 같이 정리  
   delta_k = d E / d u_k = (d E / d y_k) * (d y_k / d u_k) ----(1)
  - 손실함수와 활성화함수는 결정되지 않은 사항이므로 delta_k라 지정함  
  - 위 식을 정리하면 아래와 같음  
   *** 결론) d w_jk = y_j * delta_k  ***
   => 오차에 대한 가중치의 기울기를 y_j와 delta_k의 곱으로 표현!!  

 [3] 편향 기울기  
  - 오차 E를 가중치 b_k로 편미분한 d E / d b_k를 구함  
  - 이렇게 구한 가중치 기울기를  d b_k라 하면 다음과 표기  
   d b_k = d E / d b_k  
  - 위 식을 합성함수 미분법칙을 사용하여 다음과 같이 전개  
   d b_k = d E / d b_k = (d E / d u_k) * (d u_k / d b_k)  
  - 우변 d u_k / d b_k의 분자 u_k는 은닉층에 있는 여러 뉴런의 출력값과 가중치 곱의 합에 편향을 더한 것  
   d u_k / d b_k = d (sum(from q=1 to m)(y_q * w_qk + b_k)) / d b_k = 1  
  - 우변 왼쪽 부분, d E / d u_k 은 가중치 기울기의 경우와 마찬가지로 delta_k로 정의  
  - 위 식을 정리하면 아래와 같다  
   *** 결론) d b_k = delta_k ***  
   => 오차에 대한 편향의 기울기는 delta_k와 동일!!  
  
 [4] 입력값 기울기
  - 출력층에서 입력값은 y_j로 표현
  - 오차에 대한 y_j의 기울기 d E / d y_j를 d y_j로 표현
   d y_j = d E / d y_j
  - 합성함수 미분법에 의해 다음과 같이 전개
   d y_j = d E / d y_j = sum(from r=1 to n)((d E / d u_r) * (d u_r / d y_j))
   + sum(시그마)가 붙는 이유 : 은닉층의 뉴런 1개는 출력층 n개의 뉴런에 영향을 주기 때문!!
  - d u_r / d y_j 부분은 다음과 같이 전개
   d u_r / d y_j = d (sum(from q=1 to m)(y_q * w_qr + b_r)) / d y_j
  = d / d y_j * (y_1*w_1r + y_2*w_2r + ... + y_j*w_jr + ... + y_mw_mr + b_r) = w_jr
  - 그리고 delta_r = d E / d u_r로 설정하면 위 식은 다음과 같이 정리됨
  *** 결론) d y_j = sum(from r=1 to n) (delta_r * w_jr) ***


 참고) 출력층에서 이전 층의 출력 기울기를 구하는 이유
  오차에 대한 이전 층의 출력(현재 층의 입력) 기울기는 현재 층의 가중치 및 활성화 함수 그리고 최종 손실 함수와 관련이 있기 때문에 미리 구해서 역방향으로 전달해줘야 하기 때문


<img width="406" alt="1" src="https://github.com/user-attachments/assets/f72ae683-4de9-439b-850f-d57dd5b70856" />  <br>
<img width="374" alt="2" src="https://github.com/user-attachments/assets/dda03fae-6b5e-4ac2-b1cc-3f5ed1280255" />  <br>
<img width="370" alt="3" src="https://github.com/user-attachments/assets/508d60f7-bf65-409a-8f94-83782aad51e3" />  <br>
<img width="330" alt="4" src="https://github.com/user-attachments/assets/1e4e646d-63a0-4470-8c39-cf51fe9e6fd0" />  


  
