## 1. 배치의 개념 및 학습에 미치는 영향  
  
[1] 에포크(epoch)  
- 모든 훈련 데이터를 1회 학습하는 것을 1 epoch라 함  
- 훈련 데이터의 샘플을 여러 개 묶어 학습에 사용  
- 이 훈련 샘플 그룹을 배치(batch)라 함  
- 1 epoch는 다수의 배치로 구성  
  
[2] 배치  
<img width="380" alt="배치" src="https://github.com/user-attachments/assets/4c72819f-494f-4732-a11f-aed203266df8" />
<br>
- 배치 사이즈는 하나의 배치에 포함하는 샘플의 수  
- 하나의 배치에 포함된 모든 샘플을 사용하여 가중치와 편향을 수정  
- 배치 사이즈는 학습 중에 일정함  
  
[3] 배치 학습  
- 배치 사이즈는 전체 훈련 데이터의 수  
- 1 에포크마다 전체 훈련 데이터의 오차 평균과 개별 가중치별 기울기 합을 구함  
- 위에서 구한 평균오차와 기울기 합을 이용하여 가중치와 편향을 수정  
- 계산량이 적은 장점이 있으나 지역 최저점에 빠지기 쉬움  
- 훈련 데이터의 수를 N, 개별 데이터의 오차를 E_i라 할 경우, 오차는 다음과 같음  
E = 1/N * (sum(from i=1 to N) E_i)  
- 개별 가중치의 기울기는 다음과 같음(편향도 동일)  
∂ E / ∂ w = sum(from i=1 to N) (∂ E_i / ∂ w)  
- 훈련 데이터의 샘플 수가 1,000개일 경우, 배치 사이즈는 1,000  
- 1 에포크에 가중치와 편향은 1회 수정  
  
[4] 온라인 학습  
- 배치 사이즈가 1  
- 개별 샘플마다 가중치와 편향 수정 (<= 이전 경사하강법 로직 구현 방법)  
- Outlier의 영향을 많이 받아서 안정성이 떨어짐  
- 지역 최저점에 빠지는 것을 어느 정도 방지해 줌  
- 훈련 데이터의 샘플 수가 1,000개일 경우, 배치 사이즈가 1  
- 1 에포크에 가중치와 편향을 1,000회 수정  
  
[5] 미니 배치  
- 훈련 데이터를 작은 그룹(배치)으로 분할, 이 그룹마다 가중치와 편향을 수정  
- 온라인 학습에 비해 배치 사이즈가 크기 때문에 Outlier에 대한 민감도가 적음  
- 전체 훈련 데이터의 수를 N, 배치 사이즈를 n(n<=N)이라 할 때, 다음과 같이 정의됨.  
E = 1/n * sum(from i=1 to n) E_i  
∂ E / ∂ w = sum(from i=1 to n) (∂ E_i / ∂ w)  
- 훈련 데이터의 샘플 수가 1,000, 배치 사이즈가 50일 경우  
 => 1 에포크에 가중치와 편향을 20회 수정  
  
[6] 배치 적용  
- 배치 사이즈는 학습 시간과 성능에 영향을 줌  
- 일괄적으로 적합한 배치 사이즈는 정해져 있지 않음  
- 해결해야 할 문제별로 다양하게 배치 사이즈에 접근  
- 일반적으로 10~100 사이로 적용  
  
## 2. 순전파의 행렬 연산 표현 방법  
  
[1] 순전파  
- 은닉층에서 출력층으로 순전파 (출력이 n개 있으므로, 다중 분류로 가정)  
- 은닉층의 뉴런 수가 입력 m개, 출력 수는 n개  
- 배치 사이즈는 h  
- 입력: X, 가중치 : W라 하면  
   
 
<img width="336" alt="순전파1" src="https://github.com/user-attachments/assets/22f56d3c-16b7-4d69-b938-f0aefc04293b" />
<br>
<img width="338" alt="순전파2" src="https://github.com/user-attachments/assets/48b704aa-d354-4c6d-8326-3f9691c5352a" />
<br>

- 편향은 행 벡터, 계산시 numpy의 브로드캐스트 기능 사용  
vec(b) = (b_1, b_2, ..., b_n)  
- 편향까지 적용한 행렬
<img width="319" alt="순전파3 - 편향까지 적용한 행렬" src="https://github.com/user-attachments/assets/93ee2408-2df7-4586-8801-f8be83fa8d47" />
<br>
- 이 행렬의 각 원소는 활성화 함수 f로 처리, 출력 Y를 다음과 같이 구함
<br>
<img width="339" alt="순전파4" src="https://github.com/user-attachments/assets/fd706d4b-1780-4b5e-9d55-0d87b3835345" />
<br>
- 층의 출력 Y는 h * n, 즉, (배치 사이즈 * 출력) 뉴런 수 행렬이 된다.  
- 위의 연산을 numpy를 사용하여 정리  
- 활성화 함수는 시그모이드일 경우  
u = np.dot(x, w) + b  
y = 1 / (1+np.exp(-u))  
  
## 3. 역전파의 행렬 연산 표현 방법
  
[1] 가중치 기울기 행렬  
- δ의 행렬 Δ  
Δ =  
δ_11 δ_12 ... δ_1n  
δ_21 δ_22 ... δ_2n  
δ_h1 δ_h2 ... δ_hn  
- Δ는 층의 출력 Y와 동일하게 h*n, 즉 (배치 사이즈*뉴런 수) 행렬  
 앞서 저장한 ∂ w_jk = ∂ E / ∂ w_jk = y_j * δ_k  
- 앞 층의 출력 y_j는 현재 층이 입력과 같으므로 이후 행렬식에서 y대신 x로 표기  
- 배치 사이즈를 처리하기 위해 기울기값을 모두 더해야 함 (이전 배치 내 기울기 처리 참조)  
 ∂ E / ∂ w = sum(from p=1 to h) (∂ E_p / ∂ w_jk)  
- 앞의 내용을 반영하기 위해 배치를 적용한 가중치의 기울기 행렬 ∂ w는 다음과 같음  
<img width="296" alt="가중치기울기행렬1" src="https://github.com/user-attachments/assets/34faf7dc-b0f2-429c-9d9d-f2935019216b" />
<br>
<img width="266" alt="가중치기울기행렬2" src="https://github.com/user-attachments/assets/05cfbc17-d59e-4b18-a1b9-5edd820c77ec" />
<br>
- 이렇게 나온 결과 행렬의 각 원소는 배치 내의 기울기 총합  
- 이 행렬의 사이즈는 m*n, 즉 가중치 행렬 W의 사이즈와 동일  
- 이 행렬 곱 연산을 numpy로 구현하면 다음과 같음  
grad_w = np.dot(x.T delta)  
- grad_w : 가중치 기울기 행렬 ∂w  
- x : 입력 행렬  
- delta : δ의 행렬 Δ  
  
[2] 편향 기울기 행렬  
- 편향(bias)의 기울기는 다음식으로 구함  
∂ b_k = δ_k  
- 배치를 고려한 기울기는 δ를 배치 내에서 모두 더해야 함  
sum(from s=1 to h)(∂ E_s / ∂ b_k)  
- 이 식을 numpy로 구현하면 다음과 같음  
grad_b = np.sum(delta, axis=0)  
   
[3] 출력 기울기 행렬  
- 이전 층의 출력(현재 층의 입력) 기울기는 다음과 같이 구함  
∂ y_j = sum(from r=1 to n)(δ_r * w_jr)  
- 앞 층의 출력 y_j는 현재 층의 입력 x로 표기  
- ∂ y를 ∂ X(배치 적용)로 표현  
<img width="314" alt="출력기울기행렬1" src="https://github.com/user-attachments/assets/a56e5334-606c-4b4c-aad7-c7e1ee84fa72" />
<br>
<img width="301" alt="출력기울기행렬2" src="https://github.com/user-attachments/assets/93abc21b-e974-4a60-b0ca-c2f2fc8f0c6d" />
<br>

- Δ와 W의 행렬 곱을 위해 W^T를 사용  
- 결과 행렬의 각 원소는 현재 층에 있는 모든 뉴런의 결과의 합  
grad_x = np.dot(delta, w.T)   
  
  
- 배치 내에서는 기울기 평균을 사용하지 않고 합을 사용하는가?  
이론적으로는 기울기의 평균을 사용할 수도 있지만, 배치 내에서 평균을 사용하게 되면 기울기 값이 너무 작아서 학습 시간이 너무 길어지고, 이를 만회하기 위해서는 학습률을 과하게 높여야 할 수 있다. 그러므로 일반적으로는 배치 내에서의 기울기 합을 사용한다.  
