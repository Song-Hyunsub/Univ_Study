## 1. 회귀문제에서 손실 함수와 활성화 함수를 이용한 기울기  
  
[1] 회귀문제 설정  
 - 손실 함수 => 오차제곱합  
 - 은닉층 활성화 함수 => 시그모이드  
 - 출력층 활성화 함수 => 항등 함수  
  
[2] 출력층 기울기  
delta_k = d E / d u_k = (d E / d y_k) * (d y_k / d u_k)  
- d E / d y_k는 손실함수인 오차제곱합을 출력 y_k로 편미분해서 구함  
d E / d y_k = d / d y_k (1/2 * sum(from k=1 to n)(y_k - t_k)^2)  
= y_k - t_k  
- d y_k / d u_k은 출력층 활성화 함수를 u_k로 편미분해서 구함  
d y_k / d u_k = d u_k / d u_k = 1  
- 결국, delta_k = y_k - t_k  
- 이렇게 구한 delta_k를 이용하여 나머지 기울기 값을 구함  
d w_jk = y_j * delta_k  
d b_k = delta_k  
d y_j = sum(from r=1 to n) (delta_r * w_jr)  
  
[3] 은닉층 기울기  
delta_j = d y_j * (d y_j / d u_j)  
- 위 식의 d y_j / d u_j은 활성화 함수를 편미분해서 구함  
- 은닉층의 활성화 함수는 시그모이드 함수  
- 시그모이드 함수 f(x)의 도함수 y' = (1-y)y 임  
 => d y_j / d u_j = (1-y)y  
- 결국, delta_j는 다음과 같음  
delta_j = d y_j (1-y)y  
- 위 값을 이요하여 나머지 기울기 값을 구함  
d w_ij = y_i * delta_j  
d b_j = delta_j  
d y_i = sum(from q=1 to m)(delta_q * w_iq)  
  
## 2. 분류문제에서 손실함수와 활성화 함수를 이용한 기울기  
  
[1] 회귀문제 설정  
 - 손실 함수 => 교차 엔트로피  
 - 은닉층 활성화 함수 => 시그모이드  
 - 출력층 활성화 함수 => 소프트 맥스 함수  
  
[2] 출력층 기울기  
delta_k = d E / d u_k = (d E / d y_k) * (d y_k / d u_k)  
- 교차 엔트로피 오차 함수  
E = -sum(from k=1 to n)(t_k * log(y_k))  
- 소프트맥스 함수  
y_k = exp(u_k) / sum(from k=1 to n)(exp(u_k))  
- 교차 엔트로피 함수의 y_k 부분에 소프트맥스 함수를 대입  
E = -sum(from k=1 to n)(t_k * log(exp(u_k) / sum(from k=1 to n)(exp(u_k))))  
log p/q = log p - log q 이므로 위 식은 다음과 같이 전개됨  
E = - sum(from k=1 to n)(t_k * log(exp(u_k)) + sum(from k=1 to n)(t_k * log(sum(from k=1 to n)(exp(u_k))))  
=  - sum(from k=1 to n)(t_k * log(exp(u_k)) + sum(from k=1 to n)(t_k) * log(sum(from k=1 to n)(exp(u_k))) ------- (A)  
- 여기서 log(exp(x)) = x임 (단, 여기서의 log의 밑은 e라 가정)  
- 분류문제에서 단 하나의 정답값이 1이고 나머지는 0이므로 sum(from k=1 to n) t_k = 1
- 이전 식 (A)는 다음과 같이 정리됨  
E = -sum(from k=1 to n)(t_k * u_k) + log(sum(from k=1 to n)(exp(u_k)))  
- 위 식을 delta_k 식에 대입하면 다음과 같음  
delta_k = d E / d u_k  
= d / d u_k * (-sum(from k=1 to n)(t_k * u_k) + log(sum(from k=1 to n)(exp(u_k)))  
= -t_k + exp(u_k) / sum(from k=1 to n)(exp(u_k))  
= -t_k + y_k  
= y_k - t_k  
- 회귀 문제와 동일한 결과 산출  
delta_k = y_k - t_k  
d w_jk = y_j * delta_k  
d b_k = delta_k  
d y_j = sum(from r=1 to n)(delta_r * w_jr)  
  
[3] 은닉층 기울기  
- 은닉층의 활성화 함수는 시그모이드이므로 회귀문제와 같은 방법으로 정리  
delta_j = d y_j * (d y_j / d u_j)  
delta_j = d y_j * (1-y_j)y_j  
d w_ij = y_i * delta_j  
d b_j = delta_j  
d y_i = sum(from q=1 to m)(delta_q * w_iq)  
  
## 3. 최적화 알고리즘의 종류 및 내용  
  
[1] 최적화 알고리즘(옵티마이저)  
 - 경사 하강법은 기본적으로, 기울기를 바탕으로 가중치와 편향을 조금씩 조정  
 - 위 과정을 반복하여 정답과의 오차가 최소화되도록 신경망 최적화  
 최적화 알고리즘 => w_ij값이 전역 최저점에 도착하기 위한 구체적인 전략을 의미함  
  
[2] 확률적 경사하강법(SGD: Stochastic Gradient Descent) : 국소 최저점에 갇히는 현상을 해결한다  
 - 가중치와 편향을 수정하기 위해 반복 학습을 할 때, 전체 샘플 데이터를 사용하지 않음  
 - 무작위로 추출한 일부 샘플 데이터를 사용  
 - 가중치와 편향을 수정하는 방법은 학습률을 사용하는 기존 경사하강법과 동일  
 omega <- omega - eta (d E / d omega)  
 b <- b - eta (d E / d b)  
 - SGD 장점   
  + 전체 샘플 대신 일부 샘플을 사용하기 때문에 전체 학습시간이 단축됨  
  + 무작위로 샘플을 선택하기 때문에 지역(국소) 최저점에 잘 빠지지 않음  
 - SGD 단점  
  + 학습의 진행과정에 따른 파라미터의 수정량을 유연하게 조정할 방법이 없음  
  + 여전히 지역(국소)최저점에 빠지는 경우가 발생  
  
[3] 모멘텀(Momentum)  
 - SGD에 물리학적 관성 기법을 적용한 알고리즘  
  w = w - eta (d E / d w) + alpha * delta(w)  
  b = b - eta(d E / d b) + alpha * delta(b)  
  delta(w) : 직전 해당 가중치 파라미터의 수정량  
  delta(b) : 직전 해당 편향 파라미터의 수정량  
  alpha : 모멘텀 상수 (일반적으로 0.9 사용)  
 - 모멘텀 장점 : 기존 SGD 대비 지역(국소)최저점에 잘 빠지지 않음  
 - 모멘텀 단점 : 학습률 외에 모멘텀 상수까지, 조정해야할 파라미터가 생김  
 참고) 가중치와 편향은 파라미터라고 함, 그 외 나머지 조정해야 할 파라미터(학습률, 모멘텀 상수 등)는 하이퍼 파라미터라고 함.  
  
[4] 아다그라드(AdaGrad : Adaptive Gradient)  
 - 학습이 진행되면서 알고리즘에 의해 학습률이 감소함  
 h = h + (d E / d w)^2  
 w = w - eta (1/sqrt(h)) (d E / d w)  
 => 편향 b도 동일함  
 - 첫 번째 식에서 최초 h는 0, 학습이 진행되면서 증가  
 - 두 번째 식에서 h가 분모에 있으므로, 전체 값은 학습이 진행되면서 수정량이 감소  
 - 처음에는 넓은 영역에서 탐색을 시작, 점차 탐색범위를 좁혀가는 효율적인 동작  
 - 아다그라드 장점 : 조정해야 할 파라미터가 eta(학습률)밖에 없다.  
 - 아다그라드 단점  
  + 파라미터별로 수정량이 계속해서 감소하기 때문에 최적화 도중 수정량이 0이 되는 경우 발생  
  + 수정량이 0이 되는 해당 파라미터는 더 이상 학습이 진행되지 않음  
  
[5] RMSProp(Root Mean Square Propagation)
 - 아다그라드는 학습이 진행되면서 학습률이 0에 수렴하는 현상 발생  
  => 이러한 한계를 보완하기 위해 제프리 힌튼 교수가 발표한 알고리즘  
 - 기울기를 같은 비율로 누적하지 않고 지수이동평균을 활용  
 - 가장 최근의 기울기는 많이 반영, 먼 과거의 기울기는 조금 반영  
  
[6] Adam(ADaptive Moment Estimation)  
 - Momentum과 AdaGrad를 융합한 방법  
 - 일반적으로 좋은 성능

<br>
<br>
<br>
<br>
<br>
<img width="422" alt="1" src="https://github.com/user-attachments/assets/edeb6657-cd0c-4d24-95f4-5b8301f9d056" />
<br>
<img width="472" alt="2" src="https://github.com/user-attachments/assets/1587156f-90e0-47d7-bf52-187721535f9c" />

<br>
<img width="440" alt="3" src="https://github.com/user-attachments/assets/b2197cd8-1bad-4b49-bb4f-c29f2c63418c" />

<br>
<img width="463" alt="4" src="https://github.com/user-attachments/assets/61e57dea-964c-49a3-a540-a28774b2d82a" />

<br>
<img width="459" alt="5" src="https://github.com/user-attachments/assets/0f4c5650-391c-47d5-b994-47f5a9626a35" />
