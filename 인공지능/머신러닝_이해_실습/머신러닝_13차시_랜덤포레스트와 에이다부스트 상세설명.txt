## 앙상블 학습 (Ensemble Learning)

앙상블 학습은 여러 개의 약한 학습기(weak learner)를 결합하여 하나의 강력한 학습기(strong learner)를 만드는 기법입니다. 이를 통해 단일 모델보다 더 높은 예측 성능과 안정성을 얻을 수 있습니다. 앙상블 학습의 주요 방법으로는 배깅(Bagging)과 부스팅(Boosting)이 있습니다.

## 1. 배깅 (Bagging, Bootstrap Aggregating)

* 개념: 훈련 데이터에서 중복을 허용하는 임의의 부분 집합(bootstrap sample)을 여러 개 추출하여 각 부분 집합에 대해 동일한 알고리즘의 모델을 학습시킵니다. 이렇게 생성된 여러 모델의 예측 결과를 집계(aggregating)하여 최종 예측을 수행합니다.
* 작동 방식:
    1. 훈련 데이터에서 복원 추출(sampling with replacement)을 통해 여러 개의 부분 집합을 생성합니다.
    2. 각 부분 집합에 대해 독립적으로 모델을 학습시킵니다.
    3. 분류 문제의 경우, 다수결 투표(majority voting)를 통해 최종 분류를 결정합니다. 회귀 문제의 경우, 예측값의 평균을 계산합니다.
* 장점:
    * 데이터의 변동성에 강하며, 과적합(overfitting)을 방지하는 효과가 있습니다.
    * 다양한 데이터 샘플을 활용하여 모델의 일반화 성능을 향상시킵니다.
* 단점: 약한 모델의 성능이 낮으면 최종 결과의 성능도 낮아질 수 있습니다.
* 대표적인 예: 랜덤 포레스트(Random Forest)

## 2. 부스팅 (Boosting)

* 개념: 약한 학습기를 순차적으로 학습시키면서, 이전 모델이 잘못 예측한 데이터에 가중치를 부여하여 다음 모델이 해당 데이터를 더 잘 학습하도록 유도합니다. 즉, 오류를 개선하는 방향으로 모델을 발전시켜 나갑니다.
* 작동 방식:
    1. 모든 데이터에 동일한 가중치를 부여하여 첫 번째 모델을 학습시킵니다.
    2. 이전 모델이 잘못 예측한 데이터의 가중치를 높입니다.
    3. 높아진 가중치를 반영하여 다음 모델을 학습시킵니다.
    4. 위 과정을 반복하여 여러 개의 모델을 생성합니다.
    5. 각 모델의 예측 결과를 가중 합산하여 최종 예측을 수행합니다. 이때, 모델의 성능에 따라 가중치가 다르게 부여됩니다.
* 특징:
    * 배깅보다 더 강력한 모델을 만들 수 있습니다.
    * 오류 데이터에 집중하여 모델의 정확도를 향상시킵니다.
* 대표적인 예: 에이다부스트(AdaBoost), 경사 부스팅(Gradient Boosting), XGBoost, LightGBM

## 랜덤 포레스트 (Random Forest)

* 개념: 배깅과 결정 트리(Decision Tree)를 결합한 앙상블 학습 알고리즘입니다.
* 특징:
    * 각 트리를 학습할 때, 무작위로 선택된 변수(feature)의 부분 집합만을 고려합니다. 이를 통해 트리들 간의 상관성을 줄이고, 모델의 일반화 성능을 더욱 향상시킵니다.
    * 분류 및 회귀 문제에 모두 적용 가능하며, 군집 분석에도 활용될 수 있습니다.
* 응용 사례:
    * 소셜 네트워크의 중심성 지수 분석
    * 백화점 소비 패턴 분석을 통한 고객 연령대 예측
    * 최적의 상가 위치 분석을 통한 예상 매출액 예측
    * 고객 속성에 따른 화이트데이 선물 종류 예측

## 에이다부스트 (AdaBoost, Adaptive Boosting)

* 개념: 부스팅 알고리즘 중 하나로, 이전 모델이 잘못 분류한 데이터에 가중치를 부여하여 다음 모델이 해당 데이터를 더 잘 학습하도록 합니다.
* 특징:
    * 오류 데이터에 집중하여 모델의 성능을 향상시킵니다.
    * 각 분류기의 성능에 따라 가중치를 부여하여 최종 결과를 도출합니다.
    * 경계선 부근의 애매한 데이터에 더 많은 가중치를 부여하여 모델의 정확도를 높입니다.

## 배깅과 부스팅의 비교

| 구분 | 배깅 | 부스팅 |
|---|---|---|
| 학습 방식 | 병렬 학습 | 순차 학습 |
| 데이터 샘플링 | 복원 추출 | 이전 모델의 결과를 반영한 가중치 기반 샘플링 |
| 최종 결과 | 다수결 투표 또는 평균 | 가중치 합산 또는 가중치 투표 |
| 목표 | 분산 감소 (과적합 방지) | 편향 감소 (정확도 향상) |
